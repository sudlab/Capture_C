##############################################################################
#
#   MRC FGU CGAT
#
#   $Id$
#
#   Copyright (C) 2009 Andreas Heger
#
#   This program is free software; you can redistribute it and/or
#   modify it under the terms of the GNU General Public License
#   as published by the Free Software Foundation; either version 2
#   of the License, or (at your option) any later version.
#
#   This program is distributed in the hope that it will be useful,
#   but WITHOUT ANY WARRANTY; without even the implied warranty of
#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#   GNU General Public License for more details.
#
#   You should have received a copy of the GNU General Public License
#   along with this program; if not, write to the Free Software
#   Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
###############################################################################
"""===========================
Pipeline template
===========================

:Author: Andreas Heger
:Release: $Id$
:Date: |today|
:Tags: Python

.. Replace the documentation below with your own description of the
   pipeline's purpose

Overview
========

This pipeline computes the word frequencies in the configuration
files :file:``pipeline.ini` and :file:`conf.py`.

Usage
=====

See :ref:`PipelineSettingUp` and :ref:`PipelineRunning` on general
information how to use CGAT pipelines.

Configuration
-------------

The pipeline requires a configured :file:`pipeline.ini` file.
CGATReport report requires a :file:`conf.py` and optionally a
:file:`cgatreport.ini` file (see :ref:`PipelineReporting`).

Default configuration files can be generated by executing:

   python <srcdir>/pipeline_caputrec.py config

Input files
-----------

None required except the pipeline configuration files.

Requirements
------------

The pipeline requires the results from
:doc:`pipeline_annotations`. Set the configuration variable
:py:data:`annotations_database` and :py:data:`annotations_dir`.

On top of the default CGAT setup, the pipeline requires the following
software to be in the path:

.. Add any additional external requirements such as 3rd party software
   or R modules below:

Requirements:

* samtools >= 1.1

Pipeline output
===============

.. Describe output files of the pipeline here

Glossary
========

.. glossary::


Code
====

"""
from ruffus import *
from ruffus.combinatorics import *

import sys
import os
import re
import sqlite3
import CGAT.Experiment as E
import CGATPipelines.Pipeline as P
import CGAT.IOTools as IOTools
import PipelineCaptureC
# load options from the config file
PARAMS = P.getParameters(
    ["%s/pipeline.ini" % os.path.splitext(__file__)[0],
     "../pipeline.ini",
     "pipeline.ini"])

# add configuration values from associated pipelines
#
# 1. pipeline_annotations: any parameters will be added with the
#    prefix "annotations_". The interface will be updated with
#    "annotations_dir" to point to the absolute path names.
PARAMS.update(P.peekParameters(
    PARAMS["annotations_dir"],
    "pipeline_annotations.py",
    on_error_raise=__name__ == "__main__",
    prefix="annotations_",
    update_interface=True))



# if necessary, update the PARAMS dictionary in any modules file.
# e.g.:
#
# import CGATPipelines.PipelineGeneset as PipelineGeneset
# PipelineGeneset.PARAMS = PARAMS
#
# Note that this is a hack and deprecated, better pass all
# parameters that are needed by a function explicitely.

# -----------------------------------------------
# Utility functions
def connect():
    '''utility function to connect to database.

    Use this method to connect to the pipeline database.
    Additional databases can be attached here as well.

    Returns an sqlite3 database handle.
    '''

    dbh = sqlite3.connect(PARAMS["database"])
    statement = '''ATTACH DATABASE '%s' as annotations''' % (
        PARAMS["annotations_database"])
    cc = dbh.cursor()
    cc.execute(statement)
    cc.close()

    return dbh

###############################################################################
# Prepare Bams
###############################################################################

@follows(mkdir("dedupped.dir"))
@transform("*.bam",
           regex("(.+).bam"),
           r"dedupped.dir/\1.bam")
def dedup(infile, outfile):
    ''' Use picard to deduplicate BAM files '''

    job_memory = "5G"

    statement = ''' MarkDuplicates
                     INPUT=%(infile)s
                     OUTPUT=%(outfile)s
                     REMOVE_DUPLICATES=true
                     METRICS_FILE=%(outfile)s.metrics
                   > %(outfile)s.log '''

    P.run()


###############################################################################
@transform(dedup, suffix(".bam"),".bam.bai")
def index_deduped(infile, outfile):

    statement = '''samtools index %(infile)s'''
    P.run()

###############################################################################

###############################################################################
# Digest genome
###############################################################################
@follows(mkdir("digest.dir"))
@split(os.path.join(PARAMS["genome_dir"], "%s.fa" % PARAMS["genome"]),
       ["digest.dir/chr%s.bed.gz" % chrom for chrom in
        map(str, range(1, 23)) + ["X", "Y"]])
def splitDigest(infile, outfiles):
    '''The entire genome consumes too much memeory for EMBOSS restrict. Thus
    the genome must be digested one chormosome at a time'''

    chroms = [os.path.basename(P.snip(outfile, ".bed.gz")) for outfile in outfiles]

    statements = []

    job_memory = "3G"

    for chrom in chroms:

        enzyme = PARAMS['experiment_enzyme']
        
        statements.append('''
                          restrict
                           -sequence <(  cat %%(infile)s
                                       | python %%(scriptsdir)s/fasta2fasta.py
                                          --include="%(chrom)s$"
                                         -L digest.dir/%(chrom)s.log)
                           -enzymes %(enzyme)s
                           -sitelen 4
                           -outfile digest.dir/%(chrom)s.gff
                           -rformat gff > digest.dir/%(chrom)s.log;

                           checkpoint;

                           python %%(scriptsdir)s/gff2bed.py
                                  -I digest.dir/%(chrom)s.gff
                                  -L digest.dir/%(chrom)s.log
                                  --set-name=source
                         | awk 'BEGIN {site = 0;OFS = "\\t"}
                                      {site++; $4=site;print}'
                         | bgzip > digest.dir/%(chrom)s.bed.gz;

                           checkpoint;

                           rm digest.dir/%(chrom)s.gff''' % locals())

    P.run()


###############################################################################
@merge(splitDigest, "digest.dir/digest.bed.gz")
def mergeDigest(infiles, outfile):

    infile = " ".join(infiles)
    statement = '''zcat %(infile)s | sort -k1,1 -k2,2n | bgzip > %(outfile)s;
                   checkpoint;
                   tabix -p bed %(outfile)s'''
    P.run()


@transform(mergeDigest,
           suffix("digest.bed.gz"),
           "fragments.bed.gz")
def digest2fragments(infile, outfile):

    genome = PARAMS["annotations_interface_contigs_tsv"]
    outfile = P.snip(outfile, ".gz")
    PipelineCaptureC.sites2fragments(infile, genome, outfile)
    statement = '''uniq %(outfile)s > bgzip %(outfile)s.gz;
                   checkpoint;
                   tabix -p bed %(outfile)s.gz;
                   checkpoint;
                   rm %(outfile)s'''


###############################################################################
@transform(digest2fragments,
           suffix(".bed.gz"),
           ".load")
def loadFragments(infile, outfile):

    P.load(infile, outfile,
           options="--header=chr,start,end,name,strand,score "
                   "-i chr,start,end,name --quick")


###############################################################################
@files(["probes.bed.gz", digest2fragments],
       "probe_fragments.bed.gz")
def getProbeFragments(infiles, outfile):
    '''Fetch the fragments that contain the probes. Entries will be named for
    the probe'''

    probes, digest = infiles
    lookup = P.snip(outfile, "bed.gz") + "lookup.tsv"
    PipelineCaptureC.fetchProbeFragments(probes, digest, outfile, lookup)


###############################################################################
@transform(getProbeFragments,
           regex("(.+).bed.gz"),
           inputs(r"\1.lookup.tsv"),
           "probe_fragments_lookup.load")
def loadFragmentsLookup(infile, outfile):

    P.load(infile, outfile, options="-i probe -i fragment")


###############################################################################
@transform(getProbeFragments,
           suffix(".bed.gz"),
           ".load")
def loadProbeFragments(infile, outfile):
    
    P.load(infile, outfile, "--header=chr,start,end,name,strand,score "
                            "-i chr,start,end,name,strand, score")


###############################################################################
@follows(mkdir("probes.dir"))
@split(getProbeFragments,
       "probes.dir/*.probe")
def generateProbeSentries(infile, outfile):
    ''' Create a sentry file for each probe to allow
    combinatorial down stream tasks '''

    for line in IOTools.openFile(infile):
        P.touch("probes.dir/%s.probe" % line.split("\t")[3])

###############################################################################
@follows(loadFragments,
         loadProbeFragments,
         loadFragmentsLookup,
         generateProbeSentries)
def Prepare_fragments():
    pass


###############################################################################
# Quality Control
###############################################################################
@transform(index_deduped,
           regex("(.+).bam.bai"),
           inputs([r"\1.bam", getProbeFragments]),
           r"\1.anomolies.tsv.gz")
def qunatifyAnomileies(infiles, outfile):
    ''' Quantify level of undigested and self ligated fragments'''

    bam, probes = infiles
    PipelineCaptureC.quantifyAnomolies(bam, probes, outfile)


###############################################################################
@merge(qunatifyAnomileies, "anomolies.load")
def loadAnomolies(infiles, outfile):

    P.concatenateAndLoad(infiles, outfile,
                         regex_filename=".+/(.+).anomolies.tsv.gz",
                         options="-i track -i probe")



@transform(index_deduped,
           regex("(.+).bam.bai"),
           inputs(r"\1.bam"),
           r"\1.by_name.bam")
def reorderBAM(infile, outfile):

    prefix = P.snip(outfile, ".bam")
    statement = '''samtools sort -n -o %(outfile)s -T %(prefix)s %(infile)s
                   > %(outfile)s.log'''
    P.run()


@follows(mkdir("quantitation.dir"))
@transform(reorderBAM,
           regex(".+/(.+).by_name.bam"),
           add_inputs(digest2fragments, getProbeFragments),
           r"quantitation.dir/\1.tsv.gz")
def countInteractions(infiles, outfile):
    '''Count the number of interactions. Read pairs must map to only
    two fragments, one each for each primary alignment in the pair'''

    reads, digest, probes = infiles
    metrics = P.snip(outfile, ".tsv.gz") + ".metrics.tsv"
    PipelineCaptureC.countInteractions(reads, digest, probes,
                                       outfile, metrics,
                                       submit=True,
                                       job_memory="4G")


@merge(countInteractions, "interaction_counts.load")
def loadInteractionCounts(infiles, outfile):

    P.concatenateAndLoad(infiles, outfile,
                         regex_filename=".+/(.+).tsv.gz",
                         options = "-i track -i Frag1 -i Frag2")


@merge(countInteractions, "interaction_count_metrics.load")
def loadInteractionCountMetrics(infiles, outfile):

    infiles = [re.sub(".tsv.gz",".metrics.tsv", infile)
               for infile in infiles]
    P.concatenateAndLoad(infiles, outfile,
                         regex_filename=".+/(.+).metrics.tsv")



@follows(mkdir("bedGraphs.dir"))
@product(countInteractions,
         formatter("(.tsv.gz)$"),
         generateProbeSentries,
         formatter("(.probe)$"),
         r"bedGraphs.dir/{basename[0][0]}.{basename[1][0]}.bedGraph.gz",
         "{basename[0][0]}", "{basename[1][0]}")
def counts2bedgraph(infile, outfile, track, probe):
    '''Generate windowed bedgraph around each probe for each track'''

    track = P.snip(track, ".tsv")
    PipelineCaptureC.interactions2BedGraph(track, probe,
                                           PARAMS["database_name"],
                                           outfile)


@follows(loadInteractionCounts, loadInteractionCountMetrics,
         counts2bedgraph)
def Quant():
    pass
                      
# ---------------------------------------------------
# Generic pipeline tasks
@follows(loadAnomolies)
def full():
    pass


@follows(mkdir("report"))
def build_report():
    '''build report from scratch.

    Any existing report will be overwritten.
    '''

    E.info("starting report build process from scratch")
    P.run_report(clean=True)


@follows(mkdir("report"))
def update_report():
    '''update report.

    This will update a report with any changes inside the report
    document or code. Note that updates to the data will not cause
    relevant sections to be updated. Use the cgatreport-clean utility
    first.
    '''

    E.info("updating report")
    P.run_report(clean=False)


@follows(update_report)
def publish_report():
    '''publish report in the CGAT downloads directory.'''

    E.info("publishing report")
    P.publish_report()

if __name__ == "__main__":
    sys.exit(P.main(sys.argv))
